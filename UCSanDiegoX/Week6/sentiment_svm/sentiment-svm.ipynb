{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with support vector machines\n",
    "\n",
    "In this notebook, we will revisit a learning task that we encountered earlier in the course: predicting the *sentiment* (positive or negative) of a single sentence taken from a review of a movie, restaurant, or product. The data set consists of 3000 labeled sentences, which we divide into a training set of size 2500 and a test set of size 500. Previously we found a logistic regression classifier. Today we will use a support vector machine.\n",
    "\n",
    "Before starting on this notebook, make sure the folder `sentiment_labelled_sentences` (containing the data file `full_set.txt`) is in the same directory. Recall that the data can be downloaded from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing the data\n",
    " \n",
    "Here we follow exactly the same steps as we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  (2500, 4500)\n",
      "test data:  (500, 4500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Read in the data set.\n",
    "with open(\"sentiment_labelled_sentences/full_set.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "## Remove leading and trailing white space\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "## Separate the sentences from the labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1\n",
    "\n",
    "## full_remove takes a string x and a list of characters removal_list \n",
    "## returns x with all the characters in removal_list replaced by ' '\n",
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "digit_less = [full_remove(x, digits) for x in sentences]\n",
    "\n",
    "## Remove punctuation\n",
    "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
    "\n",
    "## Make everything lower-case\n",
    "sents_lower = [x.lower() for x in punc_less]\n",
    "\n",
    "## Define our stop words\n",
    "stop_set = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "## Remove stop words\n",
    "sents_split = [x.split() for x in sents_lower]\n",
    "sents_processed = [\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split]\n",
    "\n",
    "## Transform to bag of words representation.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 4500)\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "\n",
    "## Append '1' to the end of each vector.\n",
    "data_mat = data_features.toarray()\n",
    "\n",
    "## Split the data into testing and training sets\n",
    "np.random.seed(0)\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "\n",
    "train_data = data_mat[train_inds,]\n",
    "train_labels = y[train_inds]\n",
    "\n",
    "test_data = data_mat[test_inds,]\n",
    "test_labels = y[test_inds]\n",
    "\n",
    "print(\"train data: \", train_data.shape)\n",
    "print(\"test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting a support vector machine to the data\n",
    "\n",
    "In support vector machines, we are given a set of examples $(x_1, y_1), \\ldots, (x_n, y_n)$ and we want to find a weight vector $w \\in \\mathbb{R}^d$ that solves the following optimization problem:\n",
    "\n",
    "$$ \\min_{w \\in \\mathbb{R}^d} \\| w \\|^2 + C \\sum_{i=1}^n \\xi_i $$\n",
    "$$ \\text{subject to } y_i \\langle w, x_i \\rangle \\geq 1 - \\xi_i \\text{ for all } i=1,\\ldots, n$$\n",
    "\n",
    "`scikit-learn` provides an SVM solver that we will use. The following routine takes as input the constant `C` (from the above optimization problem) and returns the training and test error of the resulting SVM model. It is invoked as follows:\n",
    "\n",
    "* `training_error, test_error = fit_classifier(C)`\n",
    "\n",
    "The default value for parameter `C` is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def fit_classifier(C_value=1.0):\n",
    "    clf = svm.LinearSVC(C=C_value, loss='hinge')\n",
    "    clf.fit(train_data,train_labels)\n",
    "    ## Get predictions on training data\n",
    "    train_preds = clf.predict(train_data)\n",
    "    train_error = float(np.sum((train_preds > 0.0) != (train_labels > 0.0)))/len(train_labels)\n",
    "    ## Get predictions on test data\n",
    "    test_preds = clf.predict(test_data)\n",
    "    test_error = float(np.sum((test_preds > 0.0) != (test_labels > 0.0)))/len(test_labels)\n",
    "    ##\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for C = 0.01: train 0.215 test 0.250\n",
      "Error rate for C = 0.10: train 0.074 test 0.174\n",
      "Error rate for C = 1.00: train 0.011 test 0.152\n",
      "Error rate for C = 10.00: train 0.002 test 0.188\n",
      "Error rate for C = 100.00: train 0.002 test 0.196\n",
      "Error rate for C = 1000.00: train 0.003 test 0.214\n",
      "Error rate for C = 10000.00: train 0.001 test 0.206\n"
     ]
    }
   ],
   "source": [
    "cvals = [0.01,0.1,1.0,10.0,100.0,1000.0,10000.0]\n",
    "for c in cvals:\n",
    "    train_error, test_error = fit_classifier(c)\n",
    "    print (\"Error rate for C = %0.2f: train %0.3f test %0.3f\" % (c, train_error, test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluating C by k-fold cross-validation\n",
    "\n",
    "As we can see, the choice of `C` has a very significant effect on the performance of the SVM classifier. We were able to assess this because we have a separate test set. In general, however, this is a luxury we won't possess. How can we choose `C` based only on the training set?\n",
    "\n",
    "A reasonable way to estimate the error associated with a specific value of `C` is by **`k-fold cross validation`**:\n",
    "* Partition the training set `S` into `k` equal-sized sized subsets `S_1, S_2, ..., S_k`.\n",
    "* For `i=1,2,...,k`, train a classifier with parameter `C` on `S - S_i` (all the training data except `S_i`) and test it on `S_i` to get error estimate `e_i`.\n",
    "* Average the errors: `(e_1 + ... + e_k)/k`\n",
    "\n",
    "The following procedure, **cross_validation_error**, does exactly this. It takes as input:\n",
    "* the training set `x,y`\n",
    "* the value of `C` to be evaluated\n",
    "* the integer `k`\n",
    "\n",
    "and it returns the estimated error of the classifier for that particular setting of `C`. <font color=\"magenta\">Look over the code carefully to understand exactly what it is doing.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_error(x,y,C_value,k):\n",
    "    n = len(y)\n",
    "    ## Randomly shuffle indices\n",
    "    indices = np.random.permutation(n)\n",
    "    \n",
    "    ## Initialize error\n",
    "    err = 0.0\n",
    "    \n",
    "    ## Iterate over partitions\n",
    "    for i in range(k):\n",
    "        ## Partition indices\n",
    "        test_indices = indices[int(i*(n/k)):int((i+1)*(n/k) - 1)]\n",
    "        train_indices = np.setdiff1d(indices, test_indices)\n",
    "        \n",
    "        ## Train classifier with parameter c\n",
    "        clf = svm.LinearSVC(C=C_value, loss='hinge')\n",
    "        clf.fit(x[train_indices], y[train_indices])\n",
    "        \n",
    "        ## Get predictions on test partition\n",
    "        preds = clf.predict(x[test_indices])\n",
    "        \n",
    "        ## Compute error\n",
    "        err += float(np.sum((preds > 0.0) != (y[test_indices] > 0.0)))/len(test_indices)\n",
    "        \n",
    "    return err/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Picking a value of C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure **cross_validation_error** (above) evaluates a single candidate value of `C`. We need to use it repeatedly to identify a good `C`. \n",
    "\n",
    "<font color=\"magenta\">**For you to do:**</font> Write a function to choose `C`. It will be invoked as follows:\n",
    "\n",
    "* `c, err = choose_parameter(x,y,k)`\n",
    "\n",
    "where\n",
    "* `x,y` is the training data\n",
    "* `k` is the number of folds of cross-validation\n",
    "* `c` is chosen value of the parameter `C`\n",
    "* `err` is the cross-validation error estimate at `c`\n",
    "\n",
    "<font color=\"magenta\">Note:</font> This is a tricky business because a priori, even the order of magnitude of `C` is unknown. Should it be 0.0001 or 10000? You might want to think about trying multiple values that are arranged in a geometric progression (such as powers of ten). *In addition to returning a specific value of `C`, your function should **plot** the cross-validation errors for all the values of `C` it tried out (possibly using a log-scale for the `C`-axis).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameter(x,y,k):\n",
    "    ### Your code here   \n",
    "    errors = []\n",
    "    c_s = []\n",
    "    \n",
    "    for c in [10**x for x in np.arange(-10, 10, 1, dtype=float)]:\n",
    "        err = cross_validation_error(x,y,c,k)\n",
    "        errors.append(err)\n",
    "        c_s.append(c)\n",
    "        print('c: %0.1E; err: %0.3f' % (c, err))\n",
    "            \n",
    "    test_point = np.argmin(errors) - 10\n",
    "    print('Test Point', test_point)\n",
    "    \n",
    "    for c in [10**x for x in np.arange(test_point-1, test_point+1, 0.1, dtype=float)]:\n",
    "        err = cross_validation_error(x,y,c,k)\n",
    "        errors.append(err)\n",
    "        c_s.append(c)\n",
    "        print('c: %0.1E; err: %0.3f' % (c, err))\n",
    "            \n",
    "    test_point = (np.argmin(errors) - 10)*0.1\n",
    "    print('Test Point', test_point)\n",
    "    \n",
    "    for c in [10**x for x in np.arange(test_point-0.1, test_point+0.1, 0.01, dtype=float)]:\n",
    "        err = cross_validation_error(x,y,c,k)\n",
    "        errors.append(err)\n",
    "        c_s.append(c)\n",
    "        print('c: %0.1E; err: %0.3f' % (c, err))\n",
    "      \n",
    "    c = c_s[np.argmin(errors)]\n",
    "    err = np.min(errors)\n",
    "    \n",
    "    plt.semilogx(c_s, errors, 'ro')\n",
    "    plt.show\n",
    "    \n",
    "    print(c_s)\n",
    "    return c, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out your routine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 1.0E-10; err: 0.335\n",
      "c: 1.0E-09; err: 0.331\n",
      "c: 1.0E-08; err: 0.333\n",
      "c: 1.0E-07; err: 0.322\n",
      "c: 1.0E-06; err: 0.349\n",
      "c: 1.0E-05; err: 0.339\n",
      "c: 1.0E-04; err: 0.330\n",
      "c: 1.0E-03; err: 0.321\n",
      "c: 1.0E-02; err: 0.274\n",
      "c: 1.0E-01; err: 0.185\n",
      "c: 1.0E+00; err: 0.179\n",
      "c: 1.0E+01; err: 0.208\n",
      "c: 1.0E+02; err: 0.222\n",
      "c: 1.0E+03; err: 0.227\n",
      "c: 1.0E+04; err: 0.227\n",
      "c: 1.0E+05; err: 0.221\n",
      "c: 1.0E+06; err: 0.227\n",
      "c: 1.0E+07; err: 0.227\n",
      "c: 1.0E+08; err: 0.233\n",
      "c: 1.0E+09; err: 0.223\n",
      "Test Point 0\n",
      "c: 1.0E-01; err: 0.191\n",
      "c: 1.3E-01; err: 0.188\n",
      "c: 1.6E-01; err: 0.187\n",
      "c: 2.0E-01; err: 0.188\n",
      "c: 2.5E-01; err: 0.181\n",
      "c: 3.2E-01; err: 0.192\n",
      "c: 4.0E-01; err: 0.182\n",
      "c: 5.0E-01; err: 0.190\n",
      "c: 6.3E-01; err: 0.193\n",
      "c: 7.9E-01; err: 0.188\n",
      "c: 1.0E+00; err: 0.193\n",
      "c: 1.3E+00; err: 0.191\n",
      "c: 1.6E+00; err: 0.192\n",
      "c: 2.0E+00; err: 0.196\n",
      "c: 2.5E+00; err: 0.189\n",
      "c: 3.2E+00; err: 0.204\n",
      "c: 4.0E+00; err: 0.200\n",
      "c: 5.0E+00; err: 0.206\n",
      "c: 6.3E+00; err: 0.204\n",
      "c: 7.9E+00; err: 0.208\n",
      "Test Point 0.0\n",
      "c: 7.9E-01; err: 0.190\n",
      "c: 8.1E-01; err: 0.187\n",
      "c: 8.3E-01; err: 0.187\n",
      "c: 8.5E-01; err: 0.185\n",
      "c: 8.7E-01; err: 0.188\n",
      "c: 8.9E-01; err: 0.191\n",
      "c: 9.1E-01; err: 0.191\n",
      "c: 9.3E-01; err: 0.192\n",
      "c: 9.5E-01; err: 0.192\n",
      "c: 9.8E-01; err: 0.191\n",
      "c: 1.0E+00; err: 0.190\n",
      "c: 1.0E+00; err: 0.186\n",
      "c: 1.0E+00; err: 0.193\n",
      "c: 1.1E+00; err: 0.194\n",
      "c: 1.1E+00; err: 0.193\n",
      "c: 1.1E+00; err: 0.192\n",
      "c: 1.1E+00; err: 0.195\n",
      "c: 1.2E+00; err: 0.198\n",
      "c: 1.2E+00; err: 0.184\n",
      "c: 1.2E+00; err: 0.194\n",
      "[1e-10, 1.0000000000000001e-09, 1e-08, 9.9999999999999995e-08, 9.9999999999999995e-07, 1.0000000000000001e-05, 0.0001, 0.001, 0.01, 0.10000000000000001, 1.0, 10.0, 100.0, 1000.0, 10000.0, 100000.0, 1000000.0, 10000000.0, 100000000.0, 1000000000.0, 0.10000000000000001, 0.12589254117941673, 0.15848931924611134, 0.19952623149688792, 0.25118864315095796, 0.31622776601683783, 0.39810717055349715, 0.50118723362727213, 0.63095734448019303, 0.79432823472428116, 0.99999999999999944, 1.2589254117941662, 1.5848931924611125, 1.9952623149688788, 2.5118864315095784, 3.162277660168376, 3.9810717055349691, 5.0118723362727193, 6.3095734448019263, 7.9432823472428051, 0.79432823472428149, 0.81283051616409918, 0.83176377110267097, 0.85113803820237643, 0.87096358995608059, 0.89125093813374545, 0.91201083935590965, 0.93325430079699101, 0.95499258602143589, 0.97723722095581056, 0.99999999999999989, 1.0232929922807541, 1.0471285480508994, 1.0715193052376062, 1.0964781961431849, 1.1220184543019631, 1.1481536214968826, 1.1748975549395293, 1.2022644346174127, 1.2302687708123812]\n",
      "Choice of C:  1.0\n",
      "Cross-validation error estimate:  0.178714859438\n",
      "Test error:  0.152\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEECAYAAAAs+JM2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12c36be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c, err = choose_parameter(train_data, train_labels, 10)\n",
    "print(\"Choice of C: \", c)\n",
    "print(\"Cross-validation error estimate: \", err)\n",
    "## Train it and test it\n",
    "clf = svm.LinearSVC(C=c, loss='hinge')\n",
    "clf.fit(train_data, train_labels)\n",
    "preds = clf.predict(test_data)\n",
    "error = float(np.sum((preds > 0.0) != (test_labels > 0.0)))/len(test_labels)\n",
    "print(\"Test error: \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">**For you to ponder:**</font> How does the plot of cross-validation errors for different `C` look? Is there clearly a trough in which the returned value of `C` falls? Does the plot provide some reassurance that the choice is reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "344px",
    "left": "1px",
    "right": "20px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
